# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBaKhnMwpbs97xa1evQrV-9F1hgpmIUP
"""


import pandas as pd
import nltk
from nltk.corpus import stopwords
nltk.download("stopwords")
nltk.download("punkt")
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer #foreating bag of words
from sklearn.feature_extraction.text import TfidfVectorizer #creating tf-ifd
# vader
import nltk
nltk.download('vader_lexicon')
# from vader import sentiment analyzer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.metrics import accuracy_score, classification_report
#Multinomial Naive bayes
from sklearn.naive_bayes import MultinomialNB

# I use "english" because I am working on an english dataset
stop_words = stopwords.words("english")

train_dataset = pd.read_csv("sample_train.csv")

test_dataset = pd.read_csv("sample_test.csv")




# find the unique values in the label column
train_dataset["labels"].unique()

# Mapping
mapping_values = {
    "__label__1" :"negative",
    "__label__2" : "positive"
}

# mapping label column
train_dataset['labels'] = train_dataset['labels'].map(mapping_values)

test_dataset['labels'] = test_dataset['labels'].map(mapping_values)

# """**TEXT PROCESSING**"""

# remove stopwords
# bag of words
#tfidf

text = "this is an example with some stopwords, i love this product, it is good"


# tokenize the split: split into list of words
words = nltk.word_tokenize(text)

# filtering out stopwords
words = nltk.word_tokenize(text)
filtered_words = [word for word in words if word not in stop_words]


# VADER doesn't accept list, it accepts strings
# we need to convert filtered_words back to a string

# Reconstruct the list into a sentence using join method

words = nltk.word_tokenize(text)
filtered_words = [word for word in words if word not in stop_words]
filtered_text = " ". join(filtered_words)


def remove_stopwords(text):
  """
  this function takes in a sentence, tokenizes the sentence, filters out stopwords, and
  Return a more compact sentence
  """
  words = nltk.word_tokenize(text)
  filtered_words = [word for word in words if word not in stop_words]
  filtered_text = " ". join(filtered_words)
  return filtered_text

remove_stopwords(text)

train_dataset["text"].head(10).apply(remove_stopwords)

train_dataset["text"].head(10)

total_rows = len(train_dataset)

# connect tqdm to pandas
tqdm.pandas(total= total_rows)

# apply stopwords to the dataset using "progress_apply"
train_dataset['stop words']= train_dataset['text'].progress_apply(remove_stopwords)

total_rows = len(test_dataset)

# connect tqdm to pandas
tqdm.pandas(total= total_rows)

# apply stopwords to the dataset using "progress_apply"
test_dataset['stop words'] = test_dataset['text'].progress_apply(remove_stopwords)

# Creatimg bag of words from stopwords column
vectorizer = CountVectorizer()
train_bow = vectorizer.fit_transform(train_dataset['stop words'])
test_bow = vectorizer.transform(test_dataset['stop words'])

# creating a. tf-idf matrix from stopwords
tfidf_vectorizer = TfidfVectorizer()
train_tfidf = tfidf_vectorizer.fit_transform(train_dataset['stop words'])

test_idf = tfidf_vectorizer.transform(test_dataset['stop words'])

# """Modelling"""

# vader on normal sentences
# vader on sentences without stop words
# custom on train_tfidf
# custom on train_bow

# VADER is a pretrained model for analyzing sentiment of sentences

analyzer = SentimentIntensityAnalyzer()
example_text = " i hate the orange flavor, bad product"

sentiment_scores = analyzer.polarity_scores(example_text)


# we only need the compound score to know if the product is negative or positive
compound_score = sentiment_scores['compound']


# create a function using vader and sentence to get sentiment score

# """
# takes a sentence
# get the sentiments scores using analyzer

# return positive if cmp score greater than 0, else return negative
# """

def analyze_sentence(sentence):
  sentiment_scores = analyzer.polarity_scores(sentence)
  compound_score = sentiment_scores['compound']
  if compound_score > 0:
    return "positive"
  else:
    return "negative"

analyze_sentence(example_text)

#vader on text
test_dataset['vader_on_text']= test_dataset['text'].apply(analyze_sentence)

#vader on stopwords column
test_dataset['vader_on_text_without_stopwords'] = test_dataset['stop words'].apply(analyze_sentence)

# training custom models on bag of words and tf-idf

# train_tfidf

# train_bow

#creating classifier objects for tf-idf and bow model
classifier_bow = MultinomialNB()
classifier_tfidf = MultinomialNB()

classifier_bow.fit(train_bow, train_dataset['labels'])
classifier_tfidf.fit(train_tfidf, train_dataset['labels'])

test_dataset['bow']= classifier_bow.predict(test_bow)

test_dataset['tfidf'] = classifier_bow.predict(test_idf)


# """MODEL EVALUATION: USING ACCURACY SCORES AND CLASSIFICATION REPORT"""

# what are we evaluating
test_dataset.head(5)

vader_text_accuracy_score = accuracy_score(test_dataset['labels'], test_dataset['vader_on_text'])

# vader_text_accuracy_score*100

 print(classification_report(test_dataset['labels'], test_dataset['vader_on_text']))

vader_text_stopwords_accuracy_score = accuracy_score(
    test_dataset['labels'],
    test_dataset['vader_on_text_without_stopwords'])

# vader_text_stopwords_accuracy_score*100

 print(classification_report(test_dataset['labels'], test_dataset['vader_on_text_without_stopwords']))

bow_score = accuracy_score(test_dataset['labels'], test_dataset['bow'])

# bow_score*100

 print(classification_report(test_dataset['labels'], test_dataset['bow']))

tfidf_score = accuracy_score(test_dataset['labels'], test_dataset['tfidf'])

# tfidf_score*100

 print(classification_report(test_dataset['labels'], test_dataset['tfidf']))

# """**CONCLUSION:** The best performing model/pipeline is the text to stopwords remover to bow to multinomialNB"""

def inference(text):

  """
  remove stopwords.
  Convert remaining text to bag of words
  Pass the bag of words to classifier_bow
  Return prediction
  """

  filtered_text = remove_stopwords(text)
  bow_single = vectorizer.transform([filtered_text])
  inference = classifier_bow.predict(bow_single)
  return inference[0]

example = "i love the orange flavour, it is so good"

inference(example)

# Streamlit App Starts Here

import streamlit as st

st.title("Sentiment Analysis App")
st.write("This app predicts sentiment using VADER, BoW + Naive Bayes, and TF-IDF + Naive Bayes.")

user_input = st.text_area("Enter a review or sentence:")

if user_input:
    cleaned_input = remove_stopwords(user_input)

    # VADER Sentiment
    vader_sentiment = analyze_sentence(user_input)
    st.write(f"VADER Sentiment: **{vader_sentiment}**")

    # BoW + Naive Bayes
    bow_input = vectorizer.transform([cleaned_input])
    bow_pred = classifier_bow.predict(bow_input)[0]
    st.write(f"BoW + Naive Bayes Prediction: **{bow_pred}**")

    # TF-IDF + Naive Bayes
    tfidf_input = tfidf_vectorizer.transform([cleaned_input])
    tfidf_pred = classifier_tfidf.predict(tfidf_input)[0]
    st.write(f"TF-IDF + Naive Bayes Prediction: **{tfidf_pred}**")


